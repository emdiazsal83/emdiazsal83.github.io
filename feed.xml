<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://emdiazsal83.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://emdiazsal83.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-05T09:41:14+00:00</updated><id>https://emdiazsal83.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Pearl and Rubin</title><link href="https://emdiazsal83.github.io/blog/2024/pearlrubin/" rel="alternate" type="text/html" title="Pearl and Rubin"/><published>2024-06-01T23:36:10+00:00</published><updated>2024-06-01T23:36:10+00:00</updated><id>https://emdiazsal83.github.io/blog/2024/pearlrubin</id><content type="html" xml:base="https://emdiazsal83.github.io/blog/2024/pearlrubin/"><![CDATA[<p>TEST7. In the Functional Causal Model (FCM) framework of causality <d-cite key="pearl2009causality"></d-cite> [Pearl, 2009] a system is described in terms of fundamental processes involving other variables in the system, as well as random quantities. The processes chosen to describe the system must be fundamental in the sense that changes to one of the processes do not affect the others. Each process variable is modeled by an equation consisting of qualitative elements, namely the list of cause variables upon which the process depends, and quantitative elements, namely the functions describing the relationship between the process variable and its causes</p> <p>The potential outcomes framework (PO) <d-cite key="rubin1974estimating"></d-cite> [Rubin, 1974] provides a different viewpoint of the data generating mechanism to that taken by Pearl’s FCM approach. We can better grasp the FCM’s strengths and weaknesses by contrasting its conception of causal phenomena with the alternate PO viewpoint. As described, the FCM framework, in a given fixed context (observational or interventional), assumes that the data-generating mechanism involves fixed mechanisms acting on a given observational unit, mixing its cause and random characteristics to produce its effect characteristics. Barring any intervention or counterfactual scenario, the distributions of the cause and noise variables are fixed so that each unit, before it passes through the FCM mechanisms, can be characterized as an i.i.d. observation of the joint noise pdf. Equivalently, each unit, after it passes through the FCM mechanisms, can be characterized as an i.i.d. observation of the joint pdf over the FCM/DAG variables. Since it is a process point of view, the FCM requires stationarity which is guaranteed by the fact that an FCM induces a unique joint pdf over the system variables. Additionally, the modularity assumption provides structural stationarity in that changes to the system are localized and only affect certain conditional pdfs that make up the joint. In this framework, causality happens independently of individual observations at the level of the FCM equations. In the potential outcomes framework, causality happens deterministically at an individual unit level. A causal effect is defined by the difference between the intervention/observed (depending on whether the unit was subjected to a controlled experiment or a natural mechanism) outcome/effect and a counterfactual outcome/effect (this is why it is also sometimes referred to as the counterfactual framework) resulting from applying a different treatment or cause-value to the same unit. Note that here counterfactual does not mean, as with FCMs, that we change the nature of random quantities at a data-generating mechanism level. Rather it means that we assign a different cause value to a unit and then observe a deterministic outcome/effect observation for that unit. So we see in the FCM view mechanisms are stable and independent of units (modularity assumption). Causality is a property of the mechanism, i.e. the arguments of the functions \(f_i\). In contrast, in the PO view, it is units that are stable (see stable unit treatment value, or SUTVA, assumption,<d-cite key="pearl2009causality"></d-cite> [Rubin, 1978])) and causality is the process by which assigning treatments/cause-values to units changes effects values of these units. We cannot observe these individual-level causal phenomena due to the <em>fundamental causal problem</em> <d-cite key="pearl2009causality"></d-cite> [Holland, 1986].. Still, we can estimate the average effect of these micro-causal events on a population of interest under certain conditions. In this sense, the FCM framework is process-based, while the PO framework is a population-based approach. Note the randomness in the PO view comes not from noise quantities participating in the causal mechanism but from counterfactual quantities, which are deterministic but hidden: we would need to observe what effect would have been assigned to a given unit if this unit had a different cause assignment. No assumption about the causal mechanism’s stationarity seems to be made <em>a priori</em>. However, we do not need a stable, stationary causal process since we care about the average causal effect of a population of unobservable causal processes. The implicit assumption seems to be that a unit is characterized by a vector of variables relative to a population. This vector includes all the variables that are causes for <em>some</em> of the units in the population. In other words, since we only care about the average population effect, the population may contain units whose causes and effects result from heterogeneous mechanisms. Since, in this framework, the desired causal quantity to be estimated is a population property, it is important to account for the differences between the sample and the population that is the object of the causal query. Fulfilment of assumptions, such as <em>Ignorability/unconfoundedness</em> and <em>positivity/overlap</em>, determine whether the population causal quantity can be identified, even given infinite data, from the sample. These assumptions take different forms depending on whether the sample is experimental (intervention vs counterfactual definition of causality) or observational (observational vs counterfactual definition of causality). As we can see in the PO framework, there is an implicit assumption that we know which variables – the causes/treatments- are susceptible to causally affect another variable (i.e. they are causes in some of the units in the population), the effect, and which variables could confound this effect. The causal discovery problem does not exist as such in this framework. In a population of interest, the causal variables can never be observed, and in any case, they are not interesting since they may differ across units. We do not ask about the causes of a population but rather what the average effect in a population is. A strength of the PO framework is its emphasis on differences between populations. This can be incorporated into the FCM framework by adding selection variables, which indicate whether a unit of observation is selected for the sample. The augmented FCM can then account for changes between samples not captured by interventions <d-cite key="pearl2009causality"></d-cite> [Bareinboim and Pearl, 2016, Hünermund and Bareinboim, 2019]. In practice, the PO framework is used when the causes of a variable are relatively well known. The causal inference problem, then, is to estimate the causal effect, such that the focus is on potential differences between the population of interest and the sample used to estimate this causal effect. In this sense, the FCM and PO frameworks are complementary since the former focuses on identifying causes while the latter on estimating causal effects <d-cite key="pearl2009causality"></d-cite> [Colnet et al., 2020]. For Earth system sciences, the FCM framework seems to be, at least conceptually, more amenable than the PO framework since we often search for stable physical processes/laws that affect the trajectories of physical objects. In other fields, such as social sciences, where laws emerge at the population level and microscopic causal mechanisms may be inaccessible, the PO framework seems more adept at answering interesting causal questions <d-cite key="pearl2009causality"></d-cite> [VanderWeele, 2017]. .</p>]]></content><author><name></name></author><category term="causality"/><category term="causality"/><summary type="html"><![CDATA[Two intersecting frameworks for causality]]></summary></entry><entry><title type="html">Links between machine learning and causality</title><link href="https://emdiazsal83.github.io/blog/2024/MLcausality/" rel="alternate" type="text/html" title="Links between machine learning and causality"/><published>2024-06-01T15:06:00+00:00</published><updated>2024-06-01T15:06:00+00:00</updated><id>https://emdiazsal83.github.io/blog/2024/MLcausality</id><content type="html" xml:base="https://emdiazsal83.github.io/blog/2024/MLcausality/"><![CDATA[<p>Machine learning (ML), and statistics, is, in essence, a field dealing with estimating probability distributions from data. It is used within causal inference as a tool for learning observed probability distributions, or properties of these, to learn a directed acyclic graph (DAG, causal discovery) or estimate an interventional distribution (cause-effect estimation). Since causal inference is, in effect, a language that may be used to describe the changes a system may endure and articulate how these may impact the probability distribution of the system, there are many interesting links with subfields of machine learning which deal with the problem of learning from data generated in such changing environments <d-cite key="gregor2015draw"></d-cite> [Kaddour et al., 2022, Schölkopf, 2022].</p> <p>Generalisability and robustness in ML deal with learning a probability distribution from observed data that is valid for unobserved data. Insofar as any changes in the data are caused by a changing underlying generating mechanism, causality gives the language to articulate these changes and decide which parts, or factors, of the probability distribution need changing. With regard to changes resulting from differing sampling procedures, more recent developments <d-cite key="gregor2015draw"></d-cite> [Bareinboim and Pearl, 2016] also provide the necessary language to reason about what aspects of the probability distribution change and which don’t.</p> <p>Similarly, co-variate shift and non-stationarity describe situations where the data distribution of subsets of observations generated at different times has changed. In the case of a co-variate shift, where the distribution of the inputs changes, knowing the underlying causal structure can help determine if we should modify our predictive models in response to this change. If the inputs correspond to the causes and outputs to effects, then the modularity of the Functional Causal Model (FCM) dictates that this will not influence the conditional distribution of outputs given inputs. When the prediction task is anti-causal, in the sense that inputs are effects and outputs are causes, the co-variate shift does necessitate that the prediction model is changed.</p> <p>Something similar occurs in semi-supervised learning, where complementary information about the distribution of the inputs is used in learning about the conditional distribution of outputs given inputs. This may be fruitful if the modelling is done in the anti-causal direction. However, if modularity is satisfied in the causal direction, this cannot help <d-cite key="gregor2015draw"></d-cite> [Schölkopf et al., 2012].</p> <p>Transfer learning <d-cite key="gregor2015draw"></d-cite> [Pan and Yang, 2010], domain-adaptation <d-cite key="gregor2015draw"></d-cite> [Farahani et al., 2021], meta-learning <d-cite key="gregor2015draw"></d-cite> [Vilalta and Drissi, 2002], and few/one/zero-shot learning <d-cite key="gregor2015draw"></d-cite> [Wang et al., 2020] are families of methods interested in learning a probability distribution by using large amount of data generated by one distribution and then using a small amount of data generated with a changed distribution, to adapt, or update, the estimate. Again, causality can provide the language to describe these changes and the required conditions to estimate the relevant probability distributions <d-cite key="gregor2015draw"></d-cite> [Rojas-Carulla et al., 2018, Magliacane et al., 2018].</p> <p>Active learning <d-cite key="gregor2015draw"></d-cite> [Settles,2009] refers to the problem of choosing how to sample new data in order to improve our probability distribution estimation. If we can perform controlled experiments, the active learning prerogative of sampling new data is further extended. Causality gives us the tools to identify which interventional distributions to sample from, i.e., which experiments to conduct, to identify the underlying causal structure fully <d-cite key="gregor2015draw"></d-cite> [Toth et al., 2022].</p> <p>In reinforcement learning, an agent chooses actions in a stochastic environment to maximize his expected reward. Causal inference can guide the process of knowing which actions to take to better explore (sample) the different interventional distributions implied by the FCM that governs the environment <d-cite key="gregor2015draw"></d-cite> [Weichwald et al., 2022].</p> <p>In ML, latent modelling techniques, such as variational autoencoders <d-cite key="gregor2015draw"></d-cite> [Kingma and Welling, 2014], generally attempt to find a sparse underlying representation, or factorization, of a high dimensional probability distribution. This is connected to causal discovery since, under certain specific modularity assumptions, the causal factorization of the joint, corresponding to the causal DAG is the simplest possible factorization <d-cite key="gregor2015draw"></d-cite> [Schölkopf et al., 2021, Wang et al., 2023]. In ML, generative models, such as generative adversarial networks <d-cite key="gregor2015draw"></d-cite> [Goodfellow et al., 2014] or normalizing flows <d-cite key="gregor2015draw"></d-cite> [Rafajłowicz, 2020], generally attempt to describe high- dimensional structured data as a series of functions in the form of iterative transformations applied to multivariate noise, where components are mutually independent. These models effectively estimate an FCM since an FCM can be re-expressed in terms of only independent noise variables <d-cite key="gregor2015draw"></d-cite> [Monti et al., 2020, Khemakhem et al., 2020].</p> <p>Finally, hybrid and physically informed models attempt to incorporate, <em>a priori</em>, knowledge about the phenomenon at hand into ML algorithms which learn from data. From a causal point of view, this may be incorporated by keeping fixed, known subgraphs of the causal DAG. Alternatively, when mixed data from different environments are available, physics-style conservation restrictions may be introduced. In this case, several, possibly natural, interventions have occurred. Using the modularity assumption of FCMs, we may enforce those models corresponding to equations not intervened on remain invariant, such as is applied with <d-cite key="gregor2015draw"></d-cite> [Peters et al., 2016].</p>]]></content><author><name></name></author><category term="causality"/><category term="Machinelearning"/><category term="causality"/><summary type="html"><![CDATA[Causality guides ML through change]]></summary></entry></feed>